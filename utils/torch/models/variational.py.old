import inspect
import torch
import torch.nn
import utils
import utils.modules
import utils.modules.blocks
from typing import Tuple

# Check required arguments as keywords
from utils.__ops import required
from utils.__ops import check_required

class CVAE1d(utils.models.Variational):
    def __init__(self, input_channels: int = required,    input_shape: int = required, 
                       encoder_channels: list = required, mapper_mlp: list = required, 
                       latent_dimension: int = required,  decoder_channels: list = None, 
                       activation: str = required, operation_encoder: str = 'Separable1d', 
                       operation_decoder: str = 'Separable1d', **kwargs: dict):
        # Pass arguments to parent
        super(CVAE1d, self).__init__(latent_dimension)

        # 0) Check inputs
        in_dict = {
            'input_channels'    : input_channels,
            'input_shape'       : input_shape,
            'encoder_channels'  : encoder_channels,
            'mapper_mlp'        : mapper_mlp,
            'activation'        : activation,
            'operation_encoder' : operation_encoder,
            'operation_decoder' : operation_decoder,
            'latent_dimension'  : latent_dimension,
            'decoder_channels'  : decoder_channels,
        }
        check_required(self, in_dict)

        # Check decoder parameters
        if decoder_channels is None:
            decoder_channels = encoder_channels[::-1]

        # 1) Store inputs
        # encoder channels for convolutional operations
        self.input_channels         = input_channels
        self.input_shape            = input_shape
        self.mapper_mlp             = mapper_mlp
        self.activation             = activation
        self.operation_encoder      = utils.class_selector('utils.modules.blocks', operation_encoder)
        self.operation_decoder      = utils.class_selector('utils.modules.blocks', operation_decoder)
        self.encoder_channels       = [self.input_channels] + encoder_channels
        self.decoder_channels       = decoder_channels + [self.input_channels]
        self.encoder_linear_neurons = [self.encoder_channels[-1]*self.input_shape] + self.mapper_mlp
        self.decoder_linear_neurons = [self.latent_dimension] + self.mapper_mlp + [self.encoder_channels[-1]*self.input_shape]

        ########### Encoder ###########
        # Encoder convolutional operations
        encoder_operations = []
        for i in range(len(encoder_channels)-1):
            if i == 0:
                encoder_operations.append(
                    self.operation_encoder(self.encoder_channels[i], 
                        self.encoder_channels[i+1], 
                        preactivation=False, 
                        activation=self.activation, 
                        **kwargs
                    )
                )
            else:
                encoder_operations.append(
                    self.operation_encoder(self.encoder_channels[i], 
                        self.encoder_channels[i+1], 
                        activation=self.activation, 
                        **kwargs
                    )
                )
        
        # Encoder linear mapping operations
        encoder_mapping = []
        for i in range(len(self.encoder_linear_neurons)-1):
            encoder_mapping.append(
                utils.modules.blocks.Linear(
                    self.encoder_linear_neurons[i], 
                    self.encoder_linear_neurons[i+1], 
                    activation=self.activation, 
                    **kwargs
                )
            )
        
        # To sequential
        self.encoder_operations = torch.nn.Sequential(*encoder_operations)
        self.encoder_mapping = torch.nn.Sequential(*encoder_mapping)

        ########### Bottleneck ###########
        self.bottleneck_mu = utils.modules.blocks.Linear(
            self.encoder_linear_neurons[-1], 
            self.latent_dimension, 
            activation='none', 
            dropout_rate=0
        )
        self.bottleneck_logvar = utils.modules.blocks.Linear(
            self.encoder_linear_neurons[-1], 
            self.latent_dimension, 
            activation='none', 
            dropout_rate=0
        )
        
        ########### Decoder ###########
        # Decoder operations
        decoder_mapping = []
        for i in range(len(self.decoder_linear_neurons)-1):
            decoder_mapping.append(
                utils.modules.blocks.Linear(self.decoder_linear_neurons[i], 
                    self.decoder_linear_neurons[i+1], 
                    activation=self.activation, 
                    **kwargs
                )
            )

        decoder_operations = []
        for i in range(len(decoder_channels)-1):
            if i != len(self.decoder_channels)-1:
                decoder_operations.append(
                    self.operation_decoder(self.decoder_channels[i], 
                        self.decoder_channels[i+1], 
                        activation=self.activation, 
                        **kwargs
                    )
                )
            else:
                decoder_operations.append(
                    self.operation_decoder(self.decoder_channels[i], 
                        self.decoder_channels[i+1], 
                        activation='none', 
                        **kwargs
                    )
                )
        
        # To sequential
        self.decoder_operations = torch.nn.Sequential(*decoder_operations)
        self.decoder_mapping = torch.nn.Sequential(*decoder_mapping)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor,torch.Tensor]:
        z, mu, logvar = self.encode(x)
        xhat = self.decode(z)
        return xhat, mu, logvar
        
    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:
        h = self.encoder_operations(x)
        h = utils.modules.Flatten()(h)
        h = self.encoder_mapping(h)
        
        mu = self.bottleneck_mu(h)
        logvar = self.bottleneck_logvar(h)
        z = self.reparameterize(mu, logvar)
        return z, mu, logvar
        
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        h = self.decoder_mapping(z)
        h = utils.modules.UnFlatten()(h, torch.Size([h.size(0), self.decoder_channels[0], self.input_shape]))
        xhat = self.decoder_operations(h)
        return xhat

    def __str__(self):
        """To do"""
        s = ''
        return s

    def __repl__(self):
        return self.__str__()


class VAE1d(utils.models.Variational):
    """VAE model for an arbitrary number of levels"""

    def __init__(self, input_shape: int = required, encoder_neurons: list = required, 
                       latent_dimension: int = required, decoder_neurons: list = required):
        super(VAE1d, self).__init__(latent_dimension)
        
        # Check inputs
        in_dict = {
            'input_shape'      : input_shape,
            'encoder_neurons'  : encoder_neurons,
            'latent_dimension' : latent_dimension,
            'decoder_neurons'  : decoder_neurons,
        }
        check_required(self, in_dict)

        # Store input
        self.encoder_neurons    = [input_shape] + encoder_neurons
        self.decoder_neurons    = decoder_neurons + [input_shape]
        
        # Create as many linear layers as chosen
        self.encoder_levels     = [utils.modules.blocks.Linear(self.encoder_neurons[i], self.encoder_neurons[i+1]) for i in range(len(self.encoder_neurons-1))]
        self.bottleneck_mu      = torch.nn.Linear(self.encoder_neurons[-1], self.latent_dimension)
        self.bottleneck_logvar  = torch.nn.Linear(self.encoder_neurons[-1], self.latent_dimension)
        self.decoder_levels     = [utils.modules.blocks.Linear(self.decoder_neurons[i], self.decoder_neurons[i+1]) for i in range(len(self.decoder_neurons-1))]
        
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def forward(self, x: torch.Tensor):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
        
    def encode(self, x: torch.Tensor):
        h = x
        
        for i in range(len(self.encoder_levels)):
            h = self.encoder_levels[i](h)
        
        mu = self.bottleneck_mu(h)
        logvar = self.bottleneck_logvar(h)

        return mu, logvar
        
    def decode(self, x: torch.Tensor):
        h = x
        
        for i in range(len(self.decoder_levels)):
            h = self.decoder_levels[i](h)
        
        return h

    def __str__(self):
        """To do"""
        s = ''
        return s

    def __repl__(self):
        return self.__str__()



